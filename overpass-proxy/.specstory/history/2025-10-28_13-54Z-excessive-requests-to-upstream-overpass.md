<!-- Generated by SpecStory -->

<!-- cursor Session aecd3752-2cab-4dce-b53d-8a0b5d5d93ae (2025-10-28 13:54Z) -->

# Excessive requests to upstream overpass (2025-10-28 13:54Z)

_**User (2025-10-28 08:42Z)**_

is seems that therre are way tooooo many (overlapping?) requests sent to the upstream overpass to fill the cache for a simple query





---

_**Assistant (default)**_

<think><details><summary>Thought Process</summary>**Investigating project behavior**

I need to look into the project's request behavior and consider running tests or examining the code. I'll use codebase_search to explore aspects like caching, tiling, overlap merging, and rate limits. It might help to create a to-do list since this is a multi-step debugging process: I'll aim to reproduce issues, inspect the tiling assembly, and remove duplicates. First, I’ll execute broad searches for relevant terms in the codebase before diving into the files. I'll keep updates concise!</details></think>

---

I'll scan how requests are generated, tiled, deduplicated, and cached to pinpoint where overlaps can cause excessive upstream calls, then propose a fix.

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "Where are requests to the upstream Overpass API constructed and sent?" • **15** results</summary>
            
| File | Lines |
|------|-------|
| `src/upstream.ts` | L9-88 |
| `src/upstream.ts` | L31 |
| `src/interpreter.ts` | L22-120 |
| `src/tests/integration/mock-overpass.ts` | L6-71 |
| `src/interpreter.ts` | L55-166 |
| `Overpass-Proxy-Prompt.md` | L54-117 |
| `src/upstream.ts` | L20-88 |
| `specification.md` | L14 |
| `src/config.ts` | L13-36 |
| `Overpass-Proxy-Prompt.md` | L58 |
| `src/assemble.ts` | L30 |
| `README.md` | L37 |
| `specification.md` | L72-89 |
| `README.md` | L35-88 |
| `docker-compose.yml` | L16 |

</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How are queries tiled or split into bounding boxes, and are tiles deduplicated or merged?" • **15** results</summary>
            
| File | Lines |
|------|-------|
| `src/tiling.ts` | L9-20 |
| `src/interpreter.ts` | L22-120 |
| `specification.md` | L72-89 |
| `specification.md` | L14 |
| `src/assemble.ts` | L30 |
| `src/tiling.ts` | L12 |
| `src/upstream.ts` | L31 |
| `src/bbox.ts` | L9-50 |
| `Overpass-Proxy-Prompt.md` | L54-117 |
| `src/tests/unit/tiling.test.ts` | L18 |
| `src/tiling.ts` | L2-18 |
| `specification.md` | L20-74 |
| `src/interpreter.ts` | L20 |
| `src/upstream.ts` | L9-88 |
| `src/store.ts` | L39-126 |

</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How does the cache (store) key requests and prevent duplicate overlapping requests?" • **14** results</summary>
            
| File | Lines |
|------|-------|
| `src/interpreter.ts` | L22-120 |
| `specification.md` | L72-89 |
| `src/store.ts` | L39-145 |
| `src/store.ts` | L39-126 |
| `src/index.ts` | L9-86 |
| `specification.md` | L14 |
| `Overpass-Proxy-Prompt.md` | L54-117 |
| `specification.md` | L20-74 |
| `Overpass-Proxy-Prompt.md` | L58 |
| `src/interpreter.ts` | L55-166 |
| `specification.md` | L85-114 |
| `README.md` | L37 |
| `src/config.ts` | L13-36 |
| `README.md` | L35-88 |

</details>

---



---

I'll check configuration defaults (tile precision, TTL/SWR) and inspect `upstream.ts` and `tiling.ts` to confirm behavior that could cause overlapping requests, especially missing per-tile in-flight deduping. Then I'll propose a fix.

---

Tool use: **read_file**

Read file: /home/bietiekay/code/ToiletFinder/overpass-proxy/src/config.ts

---

Tool use: **read_file**

Read file: /home/bietiekay/code/ToiletFinder/overpass-proxy/src/upstream.ts

---

Tool use: **read_file**

Read file: /home/bietiekay/code/ToiletFinder/overpass-proxy/src/tiling.ts

---

<think><details><summary>Thought Process</summary>**Identifying cache issues**

I’m looking at the user's complaint about too many overlapping requests for a simple query. The problem seems to be with the interpreter.handleCacheable function, which loops sequentially to fetch missing tiles without deduplication among concurrent requests. Also, stale refreshes can cause parallel refreshes when multiple tiles are stale. There is a lock in place for refreshing per tile, but missing tiles don’t have an "acquire lock before fetching," which might lead to duplicate calls. I’ll need to implement a write-through with an in-flight lock for those missing tiles.
**Addressing tile request issues**

I'm considering creating a separate withFetchLock, like `tile:<hash>:fetch`, with a short TTL (say 30 seconds) to prevent the thundering herd problem. I wonder if the default tilePrecision of 5 might be too high and causing an excess of tiles. Although overlapping shouldn't happen due to unique tile hashes, it seems there might be issues with Overpass itself when assembling results, leading to duplicate fetches for the same tile across requests. I should implement an in-flight duplicate prevention for missing tiles by adding a `withTileLock` method in store.ts.</details></think>

---

Tool use: **read_file**

Read file: /home/bietiekay/code/ToiletFinder/overpass-proxy/src/store.ts

---

<think><details><summary>Thought Process</summary>**Evaluating overlapping requests**

I’m examining why there might be overlapping requests for tile geometry. It seems that an overpass response includes elements extending beyond each tile. Overlap can cause duplicates when combining stages, which might confuse the user, making them feel there are too many upstream requests. They might want batching to merge multiple requests into fewer ones for missing tiles by combining bounding boxes. While Overpass allows querying multiple bounding boxes, the current implementation only supports one at a time.
**Addressing stale tile refresh issues**

I’m noticing an issue with handling stale tiles—when many tiles are stale, it triggers simultaneous upstream refreshes, which could create a spike in requests. Even when responses come from cached data, the refresh still occurs for each stale tile at once. While there's a lock to prevent duplicates within a tile, overlapping requests can still lead to multiple calls. Throttling concurrency or batching refreshes could help. Also, users could adjust the TILE_PRECISION variable to reduce the number of tiles processed, making it coarser could lessen the load.
**Proposing mitigations for tile fetching**

I need to propose some mitigations for the per-tile fetch issues. First, confirming that the request scales with the number of tiles is crucial. I can recommend reducing the TILE_PRECISION and increasing CACHE_TTL_SECONDS and SWR_SECONDS to limit frequency. Introducing a per-tile fetch lock for concurrent misses is another option. We might also consider setting a concurrency limit for stale refreshes or implementing request debouncing. Lastly, logging hits and misses could provide further insights. I'll keep my recommendations concise and avoid unnecessary detail since the user hasn’t asked for implementation yet.
**Confirming symptoms and next steps**

I think I should confirm the user's symptoms with a quick question, but I don’t want to block the process. Instead, I can suggest enabling logs to verify the issue. In particular, checking the 'redis tile read' logs could provide valuable insights. After that, I can outline the next steps for them to take. This way, I’m being helpful without slowing things down!</details></think>

---

I checked how tiles are computed, cached, and fetched. The proxy issues one upstream request per missing tile; stale tiles also trigger background refreshes. There’s no in-flight dedupe for misses, and stale refreshes can fan out concurrently—both can look like “too many/overlapping” upstream requests.

### Likely causes
- Too many tiles for your bbox at current `TILE_PRECISION` (default 5).
- Concurrent clients hitting overlapping bboxes → duplicate upstream fetches for the same missing tiles (no miss-lock).
- Serving from cache but many tiles are stale → many parallel refreshes (only per-tile lock prevents duplicates, not throttling).
- Small TTL/SWR relative to traffic → frequent refreshes.

### Quick mitigations (no code changes)
- Adjust env:
  - `TILE_PRECISION=4` (coarser tiles → fewer requests)
  - `CACHE_TTL_SECONDS` higher (e.g., 86400→172800)
  - `SWR_SECONDS` higher (keep ≥ 30s; e.g., 600–1800)
  - Keep `MAX_TILES_PER_REQUEST` reasonable for your viewport
- Observe logs:
  - Look for “redis tile read” (hits/misses/stale) and “redis tile write” to confirm if spikes are from misses or stale refresh.

### Code improvements (targeted fixes)
- Add “in-flight fetch” lock for misses (like `withRefreshLock`) to coalesce concurrent requests for the same tile.
- Throttle stale refresh concurrency (e.g., limit to N concurrent refreshes/request, or global queue).
- Optional: per-request cap on refreshes (serve stale but only refresh first K tiles, rest deferred).

If you want, I can implement:
- a miss-lock in `TileStore` and use it around `fetchTile` for uncached tiles, and
- a small concurrency limiter for stale refreshes.

Let me know your preferred settings (precision/TTL/SWR), and if I should proceed with the miss-lock + refresh throttling.

- I reviewed `src/tiling.ts`, `src/interpreter.ts`, `src/store.ts`, and `src/upstream.ts`. The current design fetches per-tile, dedupes geohashes within a request, but lacks miss coalescing and refresh throttling, which can cause bursts.

---

_**User (2025-10-28 08:42Z)**_

proxy_1  | {"level":30,"time":1761661318316,"pid":1,"hostname":"168621ef02dd","method":"POST","url":"/api/interpreter","headers":{"host":"overpass-proxy.miataru.com","x-real-ip":"2003:c8:373a:e600:be24:11ff:febe:8691","access-c
ontrol-allow-origin":"*","connection":"close","content-length":"367","sec-ch-ua-platform":"\"Linux\"","user-agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36","accept"
:"*/*","sec-ch-ua":"\"Google Chrome\";v=\"141\", \"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"141\"","content-type":"application/x-www-form-urlencoded; charset=UTF-8","sec-ch-ua-mobile":"?0","origin":"https://overpass-turbo.eu","s
ec-fetch-site":"cross-site","sec-fetch-mode":"cors","sec-fetch-dest":"empty","referer":"https://overpass-turbo.eu/","accept-encoding":"gzip, deflate, br, zstd","accept-language":"en-US,en;q=0.9,ja;q=0.8"},"remoteAddress":"172.1
8.0.1","msg":"incoming request"}
proxy_1  | {"level":30,"time":1761661318316,"pid":1,"hostname":"168621ef02dd","body":{"kind":"object","size":302,"preview":"{\"data\":\"/*\\nThis has been generated by the overpass-turbo wizard.\\nThe original search was:\\n“am
enity=toilets”\\n*/\\n[out:json][timeout:25];\\n// gather results\\nnwr[\\\"amenity\\\"=\\\"toilets\\\"](22.411028521558706,112.64281675075729,23.347299312167785,113.96804258083542);\\n// print results\\nout geom;\"}"},"msg":"i
ncoming POST body"}
proxy_1  | {"level":30,"time":1761661318632,"pid":1,"hostname":"168621ef02dd","tiles":["webp7","webpk","webpm","webpq","webpr","webr2","webr3","webr6","webr7","webrk","webrm","webrq","webrr","webx2","webx3","webx6","webx7","web
xk","webxm","webxq","webxr","webz2","webz3","webz6","webz7","webzk","webzm","webzq","webzr","wecp2","wecp3","webpe","webps","webpt","webpw","webpx","webr8","webr9","webrd","webre","webrs","webrt","webrw","webrx","webx8","webx9"
,"webxd","webxe","webxs","webxt","webxw","webxx","webz8","webz9","webzd","webze","webzs","webzt","webzw","webzx","wecp8","wecp9","webpg","webpu","webpv","webpy","webpz","webrb","webrc","webrf","webrg","webru","webrv","webry","w
ebrz","webxb","webxc","webxf","webxg","webxu","webxv","webxy","webxz","webzb","webzc","webzf","webzg","webzu","webzv","webzy","webzz","wecpb","wecpc","ws005","ws00h","ws00j","ws00n","ws00p","ws020","ws021","ws024","ws025","ws02
h","ws02j","ws02n","ws02p","ws080","ws081","ws084","ws085","ws08h","ws08j","ws08n","ws08p","ws0b0","ws0b1","ws0b4","ws0b5","ws0bh","ws0bj","ws0bn","ws0bp","ws100","ws101","ws007","ws00k","ws00m","ws00q","ws00r","ws022","ws023",
"ws026","ws027","ws02k","ws02m","ws02q","ws02r","ws082","ws083","ws086","ws087","ws08k","ws08m","ws08q","ws08r","ws0b2","ws0b3","ws0b6","ws0b7","ws0bk","ws0bm","ws0bq","ws0br","ws102","ws103","ws00e","ws00s","ws00t","ws00w","ws
00x","ws028","ws029","ws02d","ws02e","ws02s","ws02t","ws02w","ws02x","ws088","ws089","ws08d","ws08e","ws08s","ws08t","ws08w","ws08x","ws0b8","ws0b9","ws0bd","ws0be","ws0bs","ws0bt","ws0bw","ws0bx","ws108","ws109","ws00g","ws00u
","ws00v","ws00y","ws00z","ws02b","ws02c","ws02f","ws02g","ws02u","ws02v","ws02y","ws02z","ws08b","ws08c","ws08f","ws08g","ws08u","ws08v","ws08y","ws08z","ws0bb","ws0bc","ws0bf","ws0bg","ws0bu","ws0bv","ws0by","ws0bz","ws10b","
ws10c","ws015","ws01h","ws01j","ws01n","ws01p","ws030","ws031","ws034","ws035","ws03h","ws03j","ws03n","ws03p","ws090","ws091","ws094","ws095","ws09h","ws09j","ws09n","ws09p","ws0c0","ws0c1","ws0c4","ws0c5","ws0ch","ws0cj","ws0
cn","ws0cp","ws110","ws111","ws017","ws01k","ws01m","ws01q","ws01r","ws032","ws033","ws036","ws037","ws03k","ws03m","ws03q","ws03r","ws092","ws093","ws096","ws097","ws09k","ws09m","ws09q","ws09r","ws0c2","ws0c3","ws0c6","ws0c7"
,"ws0ck","ws0cm","ws0cq","ws0cr","ws112","ws113","ws01e","ws01s","ws01t","ws01w","ws01x","ws038","ws039","ws03d","ws03e","ws03s","ws03t","ws03w","ws03x","ws098","ws099","ws09d","ws09e","ws09s","ws09t","ws09w","ws09x","ws0c8","w
s0c9","ws0cd","ws0ce","ws0cs","ws0ct","ws0cw","ws0cx","ws118","ws119","ws01g","ws01u","ws01v","ws01y","ws01z","ws03b","ws03c","ws03f","ws03g","ws03u","ws03v","ws03y","ws03z","ws09b","ws09c","ws09f","ws09g","ws09u","ws09v","ws09
y","ws09z","ws0cb","ws0cc","ws0cf","ws0cg","ws0cu","ws0cv","ws0cy","ws0cz","ws11b","ws11c","ws045","ws04h","ws04j","ws04n","ws04p","ws060","ws061","ws064","ws065","ws06h","ws06j","ws06n","ws06p","ws0d0","ws0d1","ws0d4","ws0d5",
"ws0dh","ws0dj","ws0dn","ws0dp","ws0f0","ws0f1","ws0f4","ws0f5","ws0fh","ws0fj","ws0fn","ws0fp","ws140","ws141","ws047","ws04k","ws04m","ws04q","ws04r","ws062","ws063","ws066","ws067","ws06k","ws06m","ws06q","ws06r","ws0d2","ws
0d3","ws0d6","ws0d7","ws0dk","ws0dm","ws0dq","ws0dr","ws0f2","ws0f3","ws0f6","ws0f7","ws0fk","ws0fm","ws0fq","ws0fr","ws142","ws143","ws04e","ws04s","ws04t","ws04w","ws04x","ws068","ws069","ws06d","ws06e","ws06s","ws06t","ws06w
","ws06x","ws0d8","ws0d9","ws0dd","ws0de","ws0ds","ws0dt","ws0dw","ws0dx","ws0f8","ws0f9","ws0fd","ws0fe","ws0fs","ws0ft","ws0fw","ws0fx","ws148","ws149","ws04g","ws04u","ws04v","ws04y","ws04z","ws06b","ws06c","ws06f","ws06g","
ws06u","ws06v","ws06y","ws06z","ws0db","ws0dc","ws0df","ws0dg","ws0du","ws0dv","ws0dy","ws0dz","ws0fb","ws0fc","ws0ff","ws0fg","ws0fu","ws0fv","ws0fy","ws0fz","ws14b","ws14c","ws055","ws05h","ws05j","ws05n","ws05p","ws070","ws0
71","ws074","ws075","ws07h","ws07j","ws07n","ws07p","ws0e0","ws0e1","ws0e4","ws0e5","ws0eh","ws0ej","ws0en","ws0ep","ws0g0","ws0g1","ws0g4","ws0g5","ws0gh","ws0gj","ws0gn","ws0gp","ws150","ws151","ws057","ws05k","ws05m","ws05q"
,"ws05r","ws072","ws073","ws076","ws077","ws07k","ws07m","ws07q","ws07r","ws0e2","ws0e3","ws0e6","ws0e7","ws0ek","ws0em","ws0eq","ws0er","ws0g2","ws0g3","ws0g6","ws0g7","ws0gk","ws0gm","ws0gq","ws0gr","ws152","ws153","ws05e","w
s05s","ws05t","ws05w","ws05x","ws078","ws079","ws07d","ws07e","ws07s","ws07t","ws07w","ws07x","ws0e8","ws0e9","ws0ed","ws0ee","ws0es","ws0et","ws0ew","ws0ex","ws0g8","ws0g9","ws0gd","ws0ge","ws0gs","ws0gt","ws0gw","ws0gx","ws15
8","ws159","ws05g","ws05u","ws05v","ws05y","ws05z","ws07b","ws07c","ws07f","ws07g","ws07u","ws07v","ws07y","ws07z","ws0eb","ws0ec","ws0ef","ws0eg","ws0eu","ws0ev","ws0ey","ws0ez","ws0gb","ws0gc","ws0gf","ws0gg","ws0gu","ws0gv",
"ws0gy","ws0gz","ws15b","ws15c","ws0h5","ws0hh","ws0hj","ws0hn","ws0hp","ws0k0","ws0k1","ws0k4","ws0k5","ws0kh","ws0kj","ws0kn","ws0kp","ws0s0","ws0s1","ws0s4","ws0s5","ws0sh","ws0sj","ws0sn","ws0sp","ws0u0","ws0u1","ws0u4","ws
0u5","ws0uh","ws0uj","ws0un","ws0up","ws1h0","ws1h1","ws0h7","ws0hk","ws0hm","ws0hq","ws0hr","ws0k2","ws0k3","ws0k6","ws0k7","ws0kk","ws0km","ws0kq","ws0kr","ws0s2","ws0s3","ws0s6","ws0s7","ws0sk","ws0sm","ws0sq","ws0sr","ws0u2
","ws0u3","ws0u6","ws0u7","ws0uk","ws0um","ws0uq","ws0ur","ws1h2","ws1h3","ws0he","ws0hs","ws0ht","ws0hw","ws0hx","ws0k8","ws0k9","ws0kd","ws0ke","ws0ks","ws0kt","ws0kw","ws0kx","ws0s8","ws0s9","ws0sd","ws0se","ws0ss","ws0st","
ws0sw","ws0sx","ws0u8","ws0u9","ws0ud","ws0ue","ws0us","ws0ut","ws0uw","ws0ux","ws1h8","ws1h9","ws0hg","ws0hu","ws0hv","ws0hy","ws0hz","ws0kb","ws0kc","ws0kf","ws0kg","ws0ku","ws0kv","ws0ky","ws0kz","ws0sb","ws0sc","ws0sf","ws0
sg","ws0su","ws0sv","ws0sy","ws0sz","ws0ub","ws0uc","ws0uf","ws0ug","ws0uu","ws0uv","ws0uy","ws0uz","ws1hb","ws1hc"],"hits":713,"misses":0,"stale":0,"msg":"redis tile read"}
proxy_1  | {"level":30,"time":1761661319634,"pid":1,"hostname":"168621ef02dd","reqId":"req-3","res":{"statusCode":200},"responseTime":1318.4225902557373,"msg":"request completed"}


---

_**Assistant (default)**_



---

- From your log, the proxy computed 713 tiles for that bbox, but it was a pure cache HIT: hits=713, misses=0, stale=0. That means no upstream Overpass requests were sent for this request.

What to change if you’re seeing upstream spikes on other requests
- Reduce tile count: set env `TILE_PRECISION=4` (coarser tiles → fewer per-request tiles).
- Increase cache stability: raise `CACHE_TTL_SECONDS` and `SWR_SECONDS` so tiles don’t go stale as often.
- Add in-flight miss dedupe: coalesce concurrent fetches for the same missing tile.
- Throttle stale refreshes: limit concurrent background refreshes per request or globally.

How to verify
- Watch for “redis tile write” logs; each corresponds to an upstream fetch.
- Optionally add a log in `fetchTile` to count upstream calls per request if you want more visibility.

If you want, I can implement miss-locking and refresh throttling, and add a lightweight counter log around `fetchTile`.

---

_**User (2025-10-28 08:42Z)**_

there were hundreds of redis tile writes

---

_**Assistant (default)**_

<think><details><summary>Thought Process</summary>**Refining tile fetching strategy**

I want to minimize long waits, so if we can't acquire a lock, I'll depend on cached data for the next request. However, that might mean missing some tile information. A better approach would be to wait with a backoff strategy if the lock isn't acquired, polling for up to 2 seconds while checking the cache. I can implement `TileStore.withFetchLock()` and `waitForTile()` to manage this efficiently, and ensure we limit outstanding refreshes to avoid duplication.
**Planning tile refresh implementation**

I'm thinking of implementing a simple `pLimit` function or perhaps using a queue for promises to keep the code straightforward. For stale tiles, I can push functions into an array and manage concurrency. Since the current setup uses `store.withRefreshLock` for each tile, I can wrap it with a limiter function in the handler. I need to add logging in `fetchTile` and create a list of tasks: miss-lock handling, polling, updating the interpreter loop, setting up a refresh limiter, and implementing the logging. Then, I’ll run `read_lints`. Let's go!</details></think>

---

I'll add a miss-lock and short wait-on-cache to coalesce concurrent tile fetches, throttle background stale refreshes per request, and add lightweight logging around upstream fetches. Then I'll wire these changes into `interpreter.ts`.

---

Tool use: **todo_write**

## Todo List

- [-] Add miss-lock and wait-on-cache to coalesce concurrent tile fetches
- [ ] Throttle per-request background stale refresh concurrency
- [ ] Add upstream fetch logging for visibility

---

I’m going to implement miss-locking in the store and integrate it in the interpreter fetch loop, then add logging around upstream fetches. I'll start by editing `src/store.ts` and `src/interpreter.ts`, and finally `src/upstream.ts`.

---

Tool use: **read_file**

Read file: /home/bietiekay/code/ToiletFinder/overpass-proxy/src/interpreter.ts

---



---

I'll add miss-locking and a helper to read a single tile in `src/store.ts`, wire miss-lock usage in `src/interpreter.ts` so concurrent requests coalesce, and add a log around upstream fetches in `src/upstream.ts`.

---

Tool use: **apply_patch**

<details>
        <summary>Apply patch for /home/bietiekay/code/ToiletFinder/overpass-proxy/src/store.ts</summary>
      

```diff
<<'PATCH'
*** Begin Patch
*** Update File: src/store.ts
@@
 export class TileStore {
   constructor(private readonly redis: Redis, private readonly options: TileStoreOptions) {}
@@
   public async writeTile(tile: TileInfo, response: OverpassResponse): Promise<void> {
@@
   }
 
+  public async readTile(tile: TileInfo): Promise<CachedTile | undefined> {
+    const key = tileKey(tile.hash);
+    const value = await this.redis.get(key);
+    if (!value) {
+      return undefined;
+    }
+    try {
+      const payload = JSON.parse(value) as OverpassTilePayload;
+      const stale = payload.expiresAt < Date.now();
+      return { tile, payload, stale };
+    } catch {
+      return undefined;
+    }
+  }
+
   public async withRefreshLock(tile: TileInfo, handler: () => Promise<void>): Promise<void> {
@@
   }
+
+  public async withMissLock(tile: TileInfo, handler: () => Promise<void>, ttlMs = 10000): Promise<'fetched' | 'waited'> {
+    const inflightKey = `${tileKey(tile.hash)}:inflight`;
+    const acquired = await this.redis.set(inflightKey, '1', 'PX', ttlMs, 'NX');
+    if (!acquired) {
+      // Another request is fetching this tile. Wait briefly for the tile to appear.
+      const deadline = Date.now() + ttlMs;
+      // simple poll loop with backoff
+      let delay = 50;
+      while (Date.now() < deadline) {
+        const existing = await this.readTile(tile);
+        if (existing) {
+          return 'waited';
+        }
+        await new Promise((r) => setTimeout(r, delay));
+        delay = Math.min(delay * 2, 400);
+      }
+      return 'waited';
+    }
+
+    try {
+      await handler();
+      return 'fetched';
+    } finally {
+      await this.redis.del(inflightKey);
+    }
+  }
 }
*** End Patch
PATCH
```

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `/home/bietiekay/code/ToiletFinder/overpass-proxy/src/store.ts`

Lint results


**No lint errors found**
</details>

---

Tool use: **apply_patch**

<details>
        <summary>Apply patch for /home/bietiekay/code/ToiletFinder/overpass-proxy/src/interpreter.ts</summary>
      

```diff
<<'PATCH'
*** Begin Patch
*** Update File: src/interpreter.ts
@@
-  const responses = [];
+  const responses = [];
+  // limit concurrent stale refreshes per request
+  const maxConcurrentRefreshes = 8;
+  let activeRefreshes = 0;
+  const refreshQueue: Array<() => void> = [];
+  const scheduleRefresh = async (fn: () => Promise<void>) => {
+    if (activeRefreshes >= maxConcurrentRefreshes) {
+      await new Promise<void>((resolve) => refreshQueue.push(resolve));
+    }
+    activeRefreshes += 1;
+    try {
+      await fn();
+    } finally {
+      activeRefreshes -= 1;
+      const next = refreshQueue.shift();
+      if (next) next();
+    }
+  };
@@
-    const cachedTile = cached.get(tile.hash);
+    const cachedTile = cached.get(tile.hash);
     if (cachedTile) {
       responses.push(cachedTile.payload.response);
       if (cachedTile.stale) {
-        void deps.store
-          .withRefreshLock(tile, async () => {
-            const response = await fetchTile(deps.config, tile.bounds);
-            await deps.store.writeTile(tile, response);
-          })
-          .catch((error) => logger.warn({ err: error }, 'failed to refresh tile'));
+        void scheduleRefresh(async () => {
+          await deps.store
+            .withRefreshLock(tile, async () => {
+              const response = await fetchTile(deps.config, tile.bounds);
+              await deps.store.writeTile(tile, response);
+            })
+            .catch((error) => logger.warn({ err: error }, 'failed to refresh tile'));
+        });
       }
       continue;
     }
 
-    const response = await fetchTile(deps.config, tile.bounds);
-    await deps.store.writeTile(tile, response);
-    responses.push(response);
+    // Miss: coalesce concurrent fetches using miss-lock; another request might be fetching it
+    const outcome = await deps.store.withMissLock(tile, async () => {
+      const response = await fetchTile(deps.config, tile.bounds);
+      await deps.store.writeTile(tile, response);
+    });
+    // After miss-lock, read from cache to use the payload (either just written or by other request)
+    const fresh = await deps.store.readTile(tile);
+    if (fresh) {
+      responses.push(fresh.payload.response);
+    } else {
+      logger.warn({ tile: tile.hash, outcome }, 'tile missing after miss-lock');
+    }
   }
*** End Patch
PATCH
```

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `/home/bietiekay/code/ToiletFinder/overpass-proxy/src/interpreter.ts`

Lint results


**No lint errors found**
</details>

---

Tool use: **apply_patch**

<details>
        <summary>Apply patch for /home/bietiekay/code/ToiletFinder/overpass-proxy/src/upstream.ts</summary>
      

```diff
<<'PATCH'
*** Begin Patch
*** Update File: src/upstream.ts
@@
 export const fetchTile = async (config: AppConfig, bbox: BoundingBox): Promise<OverpassResponse> => {
   const query = buildTileQuery(bbox);
+  logger.info({ bbox }, 'upstream fetch start');
   const response = await got.post(config.upstreamUrl, {
     body: new URLSearchParams({ data: query }).toString(),
     headers: {
       'Content-Type': 'application/x-www-form-urlencoded'
     },
     timeout: { request: 120000 }
   });
-
+  logger.info({ bbox }, 'upstream fetch done');
   return JSON.parse(response.body) as OverpassResponse;
 };
*** End Patch
PATCH
```

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `/home/bietiekay/code/ToiletFinder/overpass-proxy/src/upstream.ts`

Lint results


**No lint errors found**
</details>

---

Tool use: **todo_write**

## Todo List

- [x] Add miss-lock and wait-on-cache to coalesce concurrent tile fetches
- [x] Throttle per-request background stale refresh concurrency
- [x] Add upstream fetch logging for visibility

---

I added miss-locking to coalesce concurrent tile fetches, capped per-request stale refresh concurrency, and emitted explicit logs around upstream fetches.

- Miss coalescing: concurrent requests for the same missing tile now synchronize via a short-lived inflight lock and wait-on-cache. This should drastically reduce duplicate "redis tile write" bursts.
- Refresh throttling: stale tiles still refresh, but only up to 8 at a time per request.
- Logging: each upstream call now logs “upstream fetch start/done” with the tile bbox.

You can redeploy and watch:
- “upstream fetch …” lines (actual Overpass calls)
- “redis tile write” frequency should drop when multiple clients overlap.

- Implemented `withMissLock` and `readTile` in `src/store.ts`; wired miss-locking and per-request refresh limiter in `src/interpreter.ts`; added start/done logs in `src/upstream.ts`.

---

_**User (2025-10-28 08:42Z)**_

describe and commit

---

_**Assistant (default)**_



---

I’m going to create a commit that includes the miss-lock coalescing, per-request refresh throttling, and upstream fetch logging changes.

---

Tool use: **run_terminal_cmd**

```bash
/usr/bin/bash -lc 'cd /home/bietiekay/code/ToiletFinder/overpass-proxy && git add -A && git commit -m "feat(cache): coalesce concurrent tile fetches with miss-lock; throttle stale refreshes; add upstream fetch logs

- Add TileStore.withMissLock and readTile to dedupe concurrent misses
- Limit per-request stale refresh concurrency to 8
- Log upstream fetch start/done with bbox for visibility

This reduces duplicate upstream calls and write bursts when multiple clients hit overlapping bboxes, while keeping SWR behavior."'
```

```
[master 178346d] feat(cache): coalesce concurrent tile fetches with miss-lock; throttle stale refreshes; add upstream fetch logs
 5 files changed, 626 insertions(+), 10 deletions(-)
 create mode 100644 overpass-proxy/.specstory/history/2025-10-28_13-54Z-excessive-requests-to-upstream-overpass.md

```

---

Committed changes that reduce duplicate upstream calls and cache write bursts.

- Miss-lock coalescing: added `withMissLock` and `readTile` in `src/store.ts`; integrated in `src/interpreter.ts` to avoid parallel fetches for the same tile.
- Refresh throttling: capped per-request stale refreshes to 8 concurrent in `src/interpreter.ts`.
- Visibility: added upstream fetch start/done logs in `src/upstream.ts`.

Commit: feat(cache): coalesce concurrent tile fetches with miss-lock; throttle stale refreshes; add upstream fetch logs

- This dedupes concurrent misses and limits refresh fan-out, keeping SWR while lowering “redis tile write” spikes.

---

